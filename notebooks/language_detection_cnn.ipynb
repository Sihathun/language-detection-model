{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Language Detection Using CNN\n",
    "\n",
    "This notebook trains a Convolutional Neural Network (CNN) to recognize languages from text input.\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "**Objective:** Design and develop a CNN-based model capable of accurately identifying the language of a given text while remaining lightweight, adaptable, and easy to implement.\n",
    "\n",
    "**Key Features:**\n",
    "- Character-level CNN for language classification\n",
    "- Supports multiple languages (English, French, Spanish, Khmer, Japanese, etc.)\n",
    "- Efficient extraction of language-specific character and word patterns\n",
    "- Easy-to-use preprocessing and evaluation pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup & Installation](#1-setup--installation)\n",
    "2. [Import Libraries](#2-import-libraries)\n",
    "3. [Data Preprocessing](#3-data-preprocessing)\n",
    "4. [Dataset & DataLoader](#4-dataset--dataloader)\n",
    "5. [Model Architecture](#5-model-architecture)\n",
    "6. [Training](#6-training)\n",
    "7. [Evaluation & Visualization](#7-evaluation--visualization)\n",
    "8. [Inference - Predict Language](#8-inference---predict-language)\n",
    "9. [Usage Instructions](#9-usage-instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a04756",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation\n",
    "\n",
    "First, install the required dependencies. Run this cell only once when setting up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaec7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment and run if needed)\n",
    "# !pip install torch torchvision numpy pandas scikit-learn matplotlib seaborn tqdm pillow\n",
    "\n",
    "# Optional: Install OCR packages for text extraction from images\n",
    "# !pip install pytesseract opencv-python easyocr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8d2455",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n",
    "\n",
    "Import all necessary libraries for data processing, model building, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557b02f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üîß Using device: {device}\")\n",
    "print(f\"üì¶ PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fd388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths (adjust these if running from different location)\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # Project root\n",
    "DATA_RAW = os.path.join(BASE_DIR, \"data\", \"raw\")\n",
    "DATA_PROC = os.path.join(BASE_DIR, \"data\", \"processed\")\n",
    "MODEL_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(DATA_RAW, exist_ok=True)\n",
    "os.makedirs(DATA_PROC, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Base directory: {BASE_DIR}\")\n",
    "print(f\"üìÅ Raw data: {DATA_RAW}\")\n",
    "print(f\"üìÅ Processed data: {DATA_PROC}\")\n",
    "print(f\"üìÅ Models: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d13fde",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "### 3.1 Data Loading Functions\n",
    "\n",
    "The preprocessing pipeline:\n",
    "1. Looks for CSV files in `data/raw/` with columns (text, label) OR `.txt` files (one sample per line) named `<lang>.txt`\n",
    "2. Builds character-level vocabulary\n",
    "3. Encodes text ‚Üí fixed-length integer sequences\n",
    "4. Saves processed outputs to `data/processed/`\n",
    "\n",
    "**Supported data formats:**\n",
    "- **CSV files:** Must have columns `text` and `label`\n",
    "- **TXT files:** Named after the language (e.g., `en.txt`, `fr.txt`) with one sample per line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a7cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA PREPROCESSING FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def unicode_normalize(s):\n",
    "    \"\"\"Normalize unicode characters to NFKC form.\"\"\"\n",
    "    return unicodedata.normalize(\"NFKC\", str(s))\n",
    "\n",
    "def load_raw_data(data_raw_path):\n",
    "    \"\"\"\n",
    "    Load raw data from CSV and TXT files.\n",
    "    \n",
    "    Returns DataFrame with columns ['text', 'label'].\n",
    "    \n",
    "    Acceptable raw formats:\n",
    "      - CSV files under data/raw/ with columns text, label\n",
    "      - TXT files named lang.txt containing samples line-by-line\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    # Load CSV files\n",
    "    for csv_path in glob(os.path.join(data_raw_path, \"*.csv\")):\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, usecols=[\"text\", \"label\"])\n",
    "            rows.append(df)\n",
    "            print(f\"  ‚úÖ Loaded CSV: {os.path.basename(csv_path)} ({len(df)} samples)\")\n",
    "        except Exception:\n",
    "            # Try fallback: assume two columns without headers\n",
    "            df = pd.read_csv(csv_path, header=None, names=[\"text\", \"label\"])\n",
    "            rows.append(df)\n",
    "            print(f\"  ‚úÖ Loaded CSV (no header): {os.path.basename(csv_path)} ({len(df)} samples)\")\n",
    "    \n",
    "    # Load TXT files (each file = one language)\n",
    "    for txt_path in glob(os.path.join(data_raw_path, \"*.txt\")):\n",
    "        name = os.path.splitext(os.path.basename(txt_path))[0]\n",
    "        with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = [l.strip() for l in f if l.strip()]\n",
    "        if lines:\n",
    "            df = pd.DataFrame({\"text\": lines, \"label\": [name] * len(lines)})\n",
    "            rows.append(df)\n",
    "            print(f\"  ‚úÖ Loaded TXT: {os.path.basename(txt_path)} ({len(lines)} samples, label='{name}')\")\n",
    "    \n",
    "    if not rows:\n",
    "        return None\n",
    "    \n",
    "    df = pd.concat(rows, ignore_index=True)\n",
    "    df['text'] = df['text'].astype(str).map(unicode_normalize)\n",
    "    df['label'] = df['label'].astype(str)\n",
    "    return df\n",
    "\n",
    "def generate_sample_dataset(out_path):\n",
    "    \"\"\"\n",
    "    Generate a sample dataset for testing the pipeline.\n",
    "    In production, replace this with your actual dataset.\n",
    "    \"\"\"\n",
    "    samples = [\n",
    "        # English samples\n",
    "        (\"hello world\", \"en\"),\n",
    "        (\"this is a test\", \"en\"),\n",
    "        (\"how are you today\", \"en\"),\n",
    "        (\"machine learning is fascinating\", \"en\"),\n",
    "        (\"deep neural networks\", \"en\"),\n",
    "        (\"natural language processing\", \"en\"),\n",
    "        (\"the quick brown fox jumps\", \"en\"),\n",
    "        (\"artificial intelligence research\", \"en\"),\n",
    "        \n",
    "        # French samples\n",
    "        (\"bonjour le monde\", \"fr\"),\n",
    "        (\"je suis √©tudiant\", \"fr\"),\n",
    "        (\"comment allez-vous\", \"fr\"),\n",
    "        (\"apprentissage automatique\", \"fr\"),\n",
    "        (\"traitement du langage naturel\", \"fr\"),\n",
    "        (\"intelligence artificielle\", \"fr\"),\n",
    "        (\"bonne journ√©e √† tous\", \"fr\"),\n",
    "        (\"merci beaucoup\", \"fr\"),\n",
    "        \n",
    "        # Spanish samples\n",
    "        (\"hola mundo\", \"es\"),\n",
    "        (\"buenos d√≠as\", \"es\"),\n",
    "        (\"c√≥mo est√°s hoy\", \"es\"),\n",
    "        (\"aprendizaje autom√°tico\", \"es\"),\n",
    "        (\"procesamiento del lenguaje\", \"es\"),\n",
    "        (\"inteligencia artificial\", \"es\"),\n",
    "        (\"muchas gracias\", \"es\"),\n",
    "        (\"hasta luego amigos\", \"es\"),\n",
    "        \n",
    "        # Khmer samples\n",
    "        (\"·ûü·ûΩ·ûü·üí·ûè·û∏‚Äã·ûñ·û∑·ûó·ûñ·ûõ·üÑ·ûÄ\", \"km\"),\n",
    "        (\"·ûá·üÜ·ûö·û∂·ûî·ûü·ûΩ·ûö\", \"km\"),\n",
    "        (\"·ûü·ûº·ûò·û¢·ûö·ûÇ·ûª·ûé\", \"km\"),\n",
    "        (\"·ûö·üÄ·ûì·ûó·û∂·ûü·û∂\", \"km\"),\n",
    "        (\"·ûÄ·ûò·üí·ûñ·ûª·ûá·û∂\", \"km\"),\n",
    "        (\"·ûó·üí·ûì·üÜ·ûñ·üÅ·ûâ\", \"km\"),\n",
    "        \n",
    "        # Japanese samples\n",
    "        (\"„Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå\", \"jp\"),\n",
    "        (\"„Åä„ÅØ„Çà„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åô\", \"jp\"),\n",
    "        (\"„ÅÇ„Çä„Åå„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åô\", \"jp\"),\n",
    "        (\"Ê©üÊ¢∞Â≠¶Áøí\", \"jp\"),\n",
    "        (\"Ëá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ\", \"jp\"),\n",
    "        (\"‰∫∫Â∑•Áü•ËÉΩ\", \"jp\"),\n",
    "    ]\n",
    "    \n",
    "    df = pd.DataFrame(samples, columns=[\"text\", \"label\"])\n",
    "    df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"‚úÖ Sample dataset saved to: {out_path}\")\n",
    "    return df\n",
    "\n",
    "def build_char_vocab(texts, min_freq=1, max_vocab=None):\n",
    "    \"\"\"\n",
    "    Build character-level vocabulary from texts.\n",
    "    \n",
    "    Returns:\n",
    "        idx2char: list of characters (index -> char)\n",
    "        char2idx: dict mapping char -> index\n",
    "    \"\"\"\n",
    "    cnt = Counter()\n",
    "    for t in texts:\n",
    "        cnt.update(list(t))\n",
    "    \n",
    "    items = [c for c, f in cnt.most_common() if f >= min_freq]\n",
    "    if max_vocab:\n",
    "        items = items[:max_vocab]\n",
    "    \n",
    "    # Reserve 0 for PAD, 1 for UNK\n",
    "    idx2char = [\"<pad>\", \"<unk>\"] + items\n",
    "    char2idx = {c: i for i, c in enumerate(idx2char)}\n",
    "    \n",
    "    return idx2char, char2idx\n",
    "\n",
    "def encode_text(s, char2idx, max_len):\n",
    "    \"\"\"\n",
    "    Encode text to fixed-length integer sequence.\n",
    "    \n",
    "    Args:\n",
    "        s: input text\n",
    "        char2idx: character to index mapping\n",
    "        max_len: maximum sequence length\n",
    "    \n",
    "    Returns:\n",
    "        List of integers (padded/truncated to max_len)\n",
    "    \"\"\"\n",
    "    s = s[:max_len]\n",
    "    ids = [char2idx.get(ch, 1) for ch in s]  # UNK -> 1\n",
    "    if len(ids) < max_len:\n",
    "        ids = ids + [0] * (max_len - len(ids))  # PAD -> 0\n",
    "    return ids\n",
    "\n",
    "print(\"‚úÖ Preprocessing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59344f39",
   "metadata": {},
   "source": [
    "### 3.2 Load and Preprocess Data\n",
    "\n",
    "Run this cell to load your data and create the processed files. If no data is found, a sample dataset will be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11895c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PREPROCESSING CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Hyperparameters for preprocessing\n",
    "MAX_LEN = 128      # Maximum sequence length\n",
    "MIN_FREQ = 1       # Minimum character frequency to include in vocabulary\n",
    "MAX_VOCAB = None   # Maximum vocabulary size (None = no limit)\n",
    "\n",
    "# ============================================================\n",
    "# LOAD AND PREPROCESS DATA\n",
    "# ============================================================\n",
    "\n",
    "print(\"üìä Loading raw data...\")\n",
    "df = load_raw_data(DATA_RAW)\n",
    "\n",
    "# If no data found, generate sample dataset\n",
    "if df is None or df.empty:\n",
    "    print(\"‚ö†Ô∏è No raw data found. Generating sample dataset for testing...\")\n",
    "    sample_path = os.path.join(DATA_RAW, \"sample_data.csv\")\n",
    "    df = generate_sample_dataset(sample_path)\n",
    "    df = load_raw_data(DATA_RAW)\n",
    "\n",
    "print(f\"\\nüìà Total samples loaded: {len(df)}\")\n",
    "\n",
    "# Basic cleaning: drop empty texts\n",
    "df = df[df['text'].str.strip().astype(bool)].reset_index(drop=True)\n",
    "print(f\"üìà After cleaning: {len(df)} samples\")\n",
    "\n",
    "# Show sample distribution\n",
    "print(\"\\nüìä Label distribution:\")\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b13657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BUILD VOCABULARY AND ENCODE DATA\n",
    "# ============================================================\n",
    "\n",
    "# Build label mapping\n",
    "labels = sorted(df['label'].unique().tolist())\n",
    "label2id = {l: i for i, l in enumerate(labels)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "print(\"üè∑Ô∏è Label mapping:\")\n",
    "for label, idx in label2id.items():\n",
    "    print(f\"   {label} -> {idx}\")\n",
    "\n",
    "# Encode labels\n",
    "y = df['label'].map(label2id).astype(np.int32).values\n",
    "\n",
    "# Build character vocabulary\n",
    "texts = df['text'].astype(str).tolist()\n",
    "idx2char, char2idx = build_char_vocab(texts, min_freq=MIN_FREQ, max_vocab=MAX_VOCAB)\n",
    "\n",
    "print(f\"\\nüìù Vocabulary size: {len(idx2char)} (including PAD/UNK)\")\n",
    "print(f\"   Sample chars: {idx2char[:20]}...\")\n",
    "\n",
    "# Encode all texts\n",
    "print(\"\\n‚è≥ Encoding texts...\")\n",
    "X = np.array([encode_text(t, char2idx, MAX_LEN) for t in tqdm(texts)], dtype=np.int32)\n",
    "\n",
    "print(f\"\\n‚úÖ Encoded data shape: X={X.shape}, y={y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed78c035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE PROCESSED DATA\n",
    "# ============================================================\n",
    "\n",
    "# Save numpy arrays\n",
    "np.save(os.path.join(DATA_PROC, \"X.npy\"), X)\n",
    "np.save(os.path.join(DATA_PROC, \"y.npy\"), y)\n",
    "\n",
    "# Save vocabulary and label mapping\n",
    "with open(os.path.join(DATA_PROC, \"label_map.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(label2id, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(os.path.join(DATA_PROC, \"vocab.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(idx2char, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved processed data to: {DATA_PROC}\")\n",
    "print(f\"   - X.npy: {X.shape}\")\n",
    "print(f\"   - y.npy: {y.shape}\")\n",
    "print(f\"   - vocab.json: {len(idx2char)} characters\")\n",
    "print(f\"   - label_map.json: {len(label2id)} labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6751a91c",
   "metadata": {},
   "source": [
    "## 4. Dataset & DataLoader\n",
    "\n",
    "Create PyTorch Dataset and DataLoader for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc41a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PYTORCH DATASET CLASS\n",
    "# ============================================================\n",
    "\n",
    "class LangDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for language detection.\n",
    "    \n",
    "    Loads preprocessed data (X.npy, y.npy) and performs deterministic train/val/test split.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, split=\"train\", test_frac=0.15, val_frac=0.15, load_path=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            split: 'train', 'val', or 'test'\n",
    "            test_frac: fraction of data for testing\n",
    "            val_frac: fraction of data for validation\n",
    "            load_path: path to processed data directory\n",
    "        \"\"\"\n",
    "        if load_path is None:\n",
    "            load_path = DATA_PROC\n",
    "            \n",
    "        # Load data\n",
    "        xp = np.load(os.path.join(load_path, \"X.npy\"))\n",
    "        yp = np.load(os.path.join(load_path, \"y.npy\"))\n",
    "        \n",
    "        # Deterministic shuffle\n",
    "        rng = np.random.RandomState(42)\n",
    "        perm = rng.permutation(len(xp))\n",
    "        xp = xp[perm]\n",
    "        yp = yp[perm]\n",
    "        \n",
    "        # Calculate split sizes\n",
    "        n = len(xp)\n",
    "        n_test = int(n * test_frac)\n",
    "        n_val = int(n * val_frac)\n",
    "        n_train = n - n_test - n_val\n",
    "        \n",
    "        # Split data\n",
    "        train_X, train_y = xp[:n_train], yp[:n_train]\n",
    "        val_X, val_y = xp[n_train:n_train+n_val], yp[n_train:n_train+n_val]\n",
    "        test_X, test_y = xp[n_train+n_val:], yp[n_train+n_val:]\n",
    "        \n",
    "        if split == \"train\":\n",
    "            self.X, self.y = train_X, train_y\n",
    "        elif split == \"val\":\n",
    "            self.X, self.y = val_X, val_y\n",
    "        elif split == \"test\":\n",
    "            self.X, self.y = test_X, test_y\n",
    "        else:\n",
    "            raise ValueError(\"split must be one of: train, val, test\")\n",
    "        \n",
    "        # Load metadata\n",
    "        vocab_path = os.path.join(load_path, \"vocab.json\")\n",
    "        label_path = os.path.join(load_path, \"label_map.json\")\n",
    "        \n",
    "        self.idx2char = []\n",
    "        self.label2id = {}\n",
    "        \n",
    "        if os.path.exists(vocab_path):\n",
    "            with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                self.idx2char = json.load(f)\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                self.label2id = json.load(f)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.X[idx], dtype=torch.long)\n",
    "        y = torch.tensor(int(self.y[idx]), dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "print(\"‚úÖ LangDataset class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fab3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CREATE DATASETS AND DATALOADERS\n",
    "# ============================================================\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create datasets\n",
    "train_ds = LangDataset(split=\"train\")\n",
    "val_ds = LangDataset(split=\"val\")\n",
    "test_ds = LangDataset(split=\"test\")\n",
    "\n",
    "print(f\"üìä Dataset sizes:\")\n",
    "print(f\"   Training:   {len(train_ds)} samples\")\n",
    "print(f\"   Validation: {len(val_ds)} samples\")\n",
    "print(f\"   Testing:    {len(test_ds)} samples\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# Get vocab and label info\n",
    "vocab_size = len(train_ds.idx2char)\n",
    "num_classes = len(train_ds.label2id)\n",
    "\n",
    "print(f\"\\nüìù Vocabulary size: {vocab_size}\")\n",
    "print(f\"üè∑Ô∏è Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25d9542",
   "metadata": {},
   "source": [
    "## 5. Model Architecture\n",
    "\n",
    "### Character-level CNN for Language Detection\n",
    "\n",
    "The model uses:\n",
    "1. **Embedding Layer:** Converts character indices to dense vectors\n",
    "2. **Convolutional Layers:** Multiple kernels (3, 5, 7) to capture n-gram patterns\n",
    "3. **Max Pooling:** Extract most important features\n",
    "4. **Dropout:** Regularization to prevent overfitting\n",
    "5. **Fully Connected Layer:** Final classification\n",
    "\n",
    "```\n",
    "Input (batch, seq_len)\n",
    "    ‚Üì\n",
    "Embedding (batch, seq_len, embed_dim)\n",
    "    ‚Üì\n",
    "Transpose (batch, embed_dim, seq_len)\n",
    "    ‚Üì\n",
    "[Conv1D ‚Üí ReLU ‚Üí MaxPool] √ó 3 (different kernel sizes)\n",
    "    ‚Üì\n",
    "Concatenate\n",
    "    ‚Üì\n",
    "Dropout\n",
    "    ‚Üì\n",
    "Fully Connected ‚Üí Logits\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc1d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CHARACTER-LEVEL CNN MODEL\n",
    "# ============================================================\n",
    "\n",
    "class CharCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Character-level Convolutional Neural Network for Language Detection.\n",
    "    \n",
    "    Architecture:\n",
    "    - Embedding layer for character representations\n",
    "    - Multiple parallel Conv1D layers with different kernel sizes\n",
    "    - Global max pooling\n",
    "    - Dropout for regularization\n",
    "    - Fully connected output layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, num_classes, \n",
    "                 num_filters=128, kernel_sizes=(3, 5, 7), dropout=0.3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Size of character vocabulary (including PAD/UNK)\n",
    "            embed_dim: Embedding dimension for characters\n",
    "            num_classes: Number of language classes\n",
    "            num_filters: Number of filters per convolution\n",
    "            kernel_sizes: Tuple of kernel sizes for parallel convolutions\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Character embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # Parallel convolution layers with different kernel sizes\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=embed_dim,\n",
    "                out_channels=num_filters,\n",
    "                kernel_size=k\n",
    "            )\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "        \n",
    "        # Regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(num_filters * len(kernel_sizes), num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            Logits tensor of shape (batch, num_classes)\n",
    "        \"\"\"\n",
    "        # Embedding: (batch, seq_len) -> (batch, seq_len, embed_dim)\n",
    "        emb = self.embedding(x)\n",
    "        \n",
    "        # Transpose for Conv1d: (batch, embed_dim, seq_len)\n",
    "        emb = emb.transpose(1, 2)\n",
    "        \n",
    "        # Apply each convolution and pool\n",
    "        conv_outs = []\n",
    "        for conv in self.convs:\n",
    "            c = F.relu(conv(emb))  # (batch, num_filters, L_out)\n",
    "            c = F.max_pool1d(c, kernel_size=c.size(2)).squeeze(2)  # (batch, num_filters)\n",
    "            conv_outs.append(c)\n",
    "        \n",
    "        # Concatenate all conv outputs\n",
    "        cat = torch.cat(conv_outs, dim=1)  # (batch, num_filters * len(kernel_sizes))\n",
    "        \n",
    "        # Dropout\n",
    "        cat = self.dropout(cat)\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.fc(cat)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "print(\"‚úÖ CharCNN model class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d54f8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL CONFIGURATION & INITIALIZATION\n",
    "# ============================================================\n",
    "\n",
    "# Model hyperparameters\n",
    "EMBED_DIM = 64       # Character embedding dimension\n",
    "NUM_FILTERS = 128    # Number of convolution filters\n",
    "KERNEL_SIZES = (3, 5, 7)  # Different n-gram sizes\n",
    "DROPOUT = 0.3        # Dropout probability\n",
    "LEARNING_RATE = 1e-3 # Learning rate\n",
    "NUM_EPOCHS = 10      # Number of training epochs\n",
    "\n",
    "# Create model\n",
    "model = CharCNN(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_classes=num_classes,\n",
    "    num_filters=NUM_FILTERS,\n",
    "    kernel_sizes=KERNEL_SIZES,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(\"üß† Model Architecture:\")\n",
    "print(\"=\" * 50)\n",
    "print(model)\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nüìä Total parameters: {total_params:,}\")\n",
    "print(f\"üìä Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8234d5a",
   "metadata": {},
   "source": [
    "## 6. Training\n",
    "\n",
    "Train the CNN model with validation monitoring and early saving of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbad648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUATION FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_model(model, loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate model on a data loader.\n",
    "    \n",
    "    Returns:\n",
    "        accuracy, f1_score, true_labels, predictions\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    gold = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            logits = model(x)\n",
    "            p = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            preds.extend(p.tolist())\n",
    "            gold.extend(y.numpy().tolist())\n",
    "    \n",
    "    acc = accuracy_score(gold, preds)\n",
    "    f1 = f1_score(gold, preds, average=\"macro\")\n",
    "    \n",
    "    return acc, f1, gold, preds\n",
    "\n",
    "print(\"‚úÖ Evaluation function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cfde96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING LOOP\n",
    "# ============================================================\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "    \"val_f1\": []\n",
    "}\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
    "    for x, y in pbar:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix(loss=total_loss / (pbar.n + 1))\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # Validation phase\n",
    "    val_acc, val_f1, _, _ = evaluate_model(model, val_loader, device)\n",
    "    \n",
    "    # Log metrics\n",
    "    history[\"train_loss\"].append(avg_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    history[\"val_f1\"].append(val_f1)\n",
    "    \n",
    "    print(f\"[Epoch {epoch}] Loss: {avg_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        checkpoint = {\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"vocab\": idx2char,\n",
    "            \"label_map\": label2id,\n",
    "            \"args\": {\n",
    "                \"embed_dim\": EMBED_DIM,\n",
    "                \"num_filters\": NUM_FILTERS,\n",
    "                \"kernels\": \",\".join(map(str, KERNEL_SIZES)),\n",
    "                \"dropout\": DROPOUT\n",
    "            }\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join(MODEL_DIR, \"best_model.pt\"))\n",
    "        print(f\"  ‚úÖ Saved best model (val_acc={val_acc:.4f})\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"üéâ Training complete! Best validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13cfbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE TRAINING HISTORY\n",
    "# ============================================================\n",
    "\n",
    "# Save history to JSON\n",
    "with open(os.path.join(MODEL_DIR, \"train_history.json\"), \"w\") as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Training history saved to: {os.path.join(MODEL_DIR, 'train_history.json')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dd4797",
   "metadata": {},
   "source": [
    "## 7. Evaluation & Visualization\n",
    "\n",
    "Evaluate the trained model and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd34a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PLOT TRAINING CURVES\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Training Loss\n",
    "axes[0].plot(history[\"train_loss\"], 'b-', linewidth=2)\n",
    "axes[0].set_title(\"Training Loss\", fontsize=12)\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Accuracy\n",
    "axes[1].plot(history[\"val_acc\"], 'g-', linewidth=2)\n",
    "axes[1].set_title(\"Validation Accuracy\", fontsize=12)\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation F1 Score\n",
    "axes[2].plot(history[\"val_f1\"], 'r-', linewidth=2)\n",
    "axes[2].set_title(\"Validation F1 Score\", fontsize=12)\n",
    "axes[2].set_xlabel(\"Epoch\")\n",
    "axes[2].set_ylabel(\"F1 Score\")\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_DIR, \"training_curves.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Training curves saved to: {os.path.join(MODEL_DIR, 'training_curves.png')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938afafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD BEST MODEL AND EVALUATE ON TEST SET\n",
    "# ============================================================\n",
    "\n",
    "# Load the best checkpoint\n",
    "checkpoint_path = os.path.join(MODEL_DIR, \"best_model.pt\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "# Recreate model with saved configuration\n",
    "saved_args = checkpoint.get(\"args\", {})\n",
    "model_best = CharCNN(\n",
    "    vocab_size=len(checkpoint[\"vocab\"]),\n",
    "    embed_dim=saved_args.get(\"embed_dim\", EMBED_DIM),\n",
    "    num_classes=len(checkpoint[\"label_map\"]),\n",
    "    num_filters=saved_args.get(\"num_filters\", NUM_FILTERS),\n",
    "    kernel_sizes=tuple(map(int, saved_args.get(\"kernels\", \"3,5,7\").split(\",\"))),\n",
    "    dropout=saved_args.get(\"dropout\", DROPOUT)\n",
    ")\n",
    "model_best.load_state_dict(checkpoint[\"model_state\"])\n",
    "model_best = model_best.to(device)\n",
    "model_best.eval()\n",
    "\n",
    "# Evaluate on test set\n",
    "test_acc, test_f1, gold, preds = evaluate_model(model_best, test_loader, device)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"üìä TEST SET RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"   Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"   F1 Score: {test_f1:.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656f5412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CLASSIFICATION REPORT\n",
    "# ============================================================\n",
    "\n",
    "# Get label names\n",
    "id2label_saved = {int(v): k for k, v in checkpoint[\"label_map\"].items()}\n",
    "label_names = [id2label_saved[i] for i in range(len(id2label_saved))]\n",
    "\n",
    "print(\"üìã CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 50)\n",
    "print(classification_report(gold, preds, target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12261315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFUSION MATRIX VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "cm = confusion_matrix(gold, preds)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt=\"d\", \n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=label_names,\n",
    "    yticklabels=label_names,\n",
    "    cbar_kws={'label': 'Count'}\n",
    ")\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "plt.ylabel(\"True Label\", fontsize=12)\n",
    "plt.title(\"Confusion Matrix - Language Detection CNN\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_DIR, \"confusion_matrix.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Confusion matrix saved to: {os.path.join(MODEL_DIR, 'confusion_matrix.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fab4ba9",
   "metadata": {},
   "source": [
    "## 8. Inference - Predict Language\n",
    "\n",
    "Use the trained model to predict the language of new text inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2426c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PREDICTION FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "class LanguagePredictor:\n",
    "    \"\"\"\n",
    "    A class for making language predictions with the trained CNN model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_path, max_len=128):\n",
    "        \"\"\"\n",
    "        Load model from checkpoint.\n",
    "        \n",
    "        Args:\n",
    "            checkpoint_path: Path to the saved model checkpoint\n",
    "            max_len: Maximum sequence length\n",
    "        \"\"\"\n",
    "        self.max_len = max_len\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Load checkpoint\n",
    "        self.checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        self.idx2char = self.checkpoint[\"vocab\"]\n",
    "        self.label_map = self.checkpoint[\"label_map\"]\n",
    "        self.id2label = {int(v): k for k, v in self.label_map.items()}\n",
    "        self.char2idx = {c: i for i, c in enumerate(self.idx2char)}\n",
    "        \n",
    "        # Build model\n",
    "        args = self.checkpoint.get(\"args\", {})\n",
    "        self.model = CharCNN(\n",
    "            vocab_size=len(self.idx2char),\n",
    "            embed_dim=args.get(\"embed_dim\", 64),\n",
    "            num_classes=len(self.label_map),\n",
    "            num_filters=args.get(\"num_filters\", 128),\n",
    "            kernel_sizes=tuple(map(int, args.get(\"kernels\", \"3,5,7\").split(\",\"))),\n",
    "            dropout=args.get(\"dropout\", 0.3)\n",
    "        )\n",
    "        self.model.load_state_dict(self.checkpoint[\"model_state\"])\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text to integer sequence.\"\"\"\n",
    "        text = unicodedata.normalize(\"NFKC\", str(text))\n",
    "        ids = [self.char2idx.get(ch, 1) for ch in text[:self.max_len]]\n",
    "        if len(ids) < self.max_len:\n",
    "            ids = ids + [0] * (self.max_len - len(ids))\n",
    "        return ids\n",
    "    \n",
    "    def predict(self, text, top_k=3):\n",
    "        \"\"\"\n",
    "        Predict language for input text.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text string\n",
    "            top_k: Number of top predictions to return\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (predicted_language, confidence, all_probabilities)\n",
    "        \"\"\"\n",
    "        ids = torch.tensor([self.encode(text)], dtype=torch.long).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = self.model(ids)\n",
    "            probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "        \n",
    "        # Get top-k predictions\n",
    "        top_indices = probs.argsort()[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                \"language\": self.id2label[idx],\n",
    "                \"confidence\": float(probs[idx])\n",
    "            })\n",
    "        \n",
    "        return results[0][\"language\"], results[0][\"confidence\"], results\n",
    "\n",
    "print(\"‚úÖ LanguagePredictor class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccadb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST PREDICTIONS\n",
    "# ============================================================\n",
    "\n",
    "# Initialize predictor\n",
    "predictor = LanguagePredictor(os.path.join(MODEL_DIR, \"best_model.pt\"))\n",
    "\n",
    "# Test samples\n",
    "test_samples = [\n",
    "    \"Hello, how are you doing today?\",\n",
    "    \"Bonjour, comment allez-vous?\",\n",
    "    \"Hola, ¬øc√≥mo est√°s?\",\n",
    "    \"·ûü·ûΩ·ûü·üí·ûè·û∏‚Äã·ûñ·û∑·ûó·ûñ·ûõ·üÑ·ûÄ\",\n",
    "    \"„Åì„Çì„Å´„Å°„ÅØ„ÄÅÂÖÉÊ∞ó„Åß„Åô„ÅãÔºü\",\n",
    "    \"Machine learning is amazing\",\n",
    "    \"L'intelligence artificielle\",\n",
    "]\n",
    "\n",
    "print(\"üîÆ LANGUAGE PREDICTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for sample in test_samples:\n",
    "    lang, conf, top_results = predictor.predict(sample, top_k=3)\n",
    "    print(f\"\\nüìù Input: \\\"{sample}\\\"\")\n",
    "    print(f\"   ‚ûú Predicted: {lang.upper()} (confidence: {conf:.2%})\")\n",
    "    # Format top 3 results\n",
    "    top3_str = \", \".join([f\"{r['language']}:{r['confidence']:.2%}\" for r in top_results])\n",
    "    print(f\"   Top 3: {top3_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc7108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INTERACTIVE PREDICTION\n",
    "# ============================================================\n",
    "\n",
    "def predict_language(text):\n",
    "    \"\"\"\n",
    "    Simple function to predict language of input text.\n",
    "    \n",
    "    Usage:\n",
    "        predict_language(\"Hello world\")\n",
    "    \"\"\"\n",
    "    lang, conf, results = predictor.predict(text)\n",
    "    print(f\"üåê Language: {lang.upper()}\")\n",
    "    print(f\"üìä Confidence: {conf:.2%}\")\n",
    "    print(f\"üìà All probabilities:\")\n",
    "    for r in results:\n",
    "        bar = \"‚ñà\" * int(r[\"confidence\"] * 20)\n",
    "        print(f\"   {r['language']:>5}: {bar} {r['confidence']:.2%}\")\n",
    "    return lang, conf\n",
    "\n",
    "# Try it yourself! Change the text below:\n",
    "predict_language(\"This is a test sentence in English\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ee94ce",
   "metadata": {},
   "source": [
    "## 9. Usage Instructions\n",
    "\n",
    "### üìÅ Project Structure\n",
    "```\n",
    "Deep Learning/\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ raw/           # Place your raw data files here\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ *.csv      # CSV with columns: text, label\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ *.txt      # TXT files named <language>.txt\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ processed/     # Preprocessed data (auto-generated)\n",
    "‚îú‚îÄ‚îÄ models/            # Saved models and artifacts\n",
    "‚îú‚îÄ‚îÄ notebooks/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ language_detection_cnn.ipynb\n",
    "‚îú‚îÄ‚îÄ src/               # Source code modules\n",
    "‚îî‚îÄ‚îÄ requirements.txt\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Quick Start Guide\n",
    "\n",
    "#### Step 1: Prepare Your Data\n",
    "Place your data in `data/raw/` in one of these formats:\n",
    "\n",
    "**Option A: CSV files**\n",
    "```csv\n",
    "text,label\n",
    "Hello world,en\n",
    "Bonjour le monde,fr\n",
    "Hola mundo,es\n",
    "```\n",
    "\n",
    "**Option B: TXT files (one per language)**\n",
    "- `en.txt` - One English sample per line\n",
    "- `fr.txt` - One French sample per line\n",
    "- etc.\n",
    "\n",
    "#### Step 2: Run the Notebook\n",
    "Execute cells in order:\n",
    "1. **Cell 1-2**: Install dependencies & import libraries\n",
    "2. **Cell 3-6**: Preprocess data (builds vocabulary, encodes text)\n",
    "3. **Cell 7-8**: Create datasets and dataloaders\n",
    "4. **Cell 9-10**: Define model architecture\n",
    "5. **Cell 11-13**: Train the model\n",
    "6. **Cell 14-17**: Evaluate and visualize results\n",
    "7. **Cell 18-20**: Make predictions on new text\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Hyperparameters\n",
    "\n",
    "| Parameter | Default | Description |\n",
    "|-----------|---------|-------------|\n",
    "| `MAX_LEN` | 128 | Maximum text sequence length |\n",
    "| `BATCH_SIZE` | 32 | Training batch size |\n",
    "| `EMBED_DIM` | 64 | Character embedding dimension |\n",
    "| `NUM_FILTERS` | 128 | CNN filter count |\n",
    "| `KERNEL_SIZES` | (3, 5, 7) | N-gram sizes to capture |\n",
    "| `DROPOUT` | 0.3 | Dropout rate |\n",
    "| `LEARNING_RATE` | 0.001 | Adam optimizer learning rate |\n",
    "| `NUM_EPOCHS` | 10 | Training epochs |\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Adding More Languages\n",
    "\n",
    "1. Add training data to `data/raw/`:\n",
    "   - CSV: Add rows with new language label\n",
    "   - TXT: Create `<lang_code>.txt` file\n",
    "\n",
    "2. Re-run preprocessing cells (Section 3)\n",
    "\n",
    "3. Re-train the model (Section 6)\n",
    "\n",
    "---\n",
    "\n",
    "### üíæ Using the Trained Model\n",
    "\n",
    "```python\n",
    "# Load the trained model\n",
    "predictor = LanguagePredictor(\"models/best_model.pt\")\n",
    "\n",
    "# Predict language\n",
    "language, confidence, all_results = predictor.predict(\"Your text here\")\n",
    "print(f\"Language: {language}, Confidence: {confidence:.2%}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Command Line Usage\n",
    "\n",
    "You can also use the source files directly:\n",
    "\n",
    "```bash\n",
    "# Preprocess data\n",
    "python src/preprocess.py --max-len 128\n",
    "\n",
    "# Train model\n",
    "python src/train.py --epochs 10 --batch-size 64\n",
    "\n",
    "# Evaluate/predict\n",
    "python src/evaluate.py --text \"Hello world\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Recommended Datasets\n",
    "\n",
    "For production use, consider these multilingual datasets:\n",
    "- **Tatoeba**: Sentence translations in 300+ languages\n",
    "- **Wikipedia**: Text dumps for most languages\n",
    "- **WMT**: Machine translation dataset\n",
    "- **OPUS**: Parallel corpus collection\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Tips for Better Performance\n",
    "\n",
    "1. **More data**: Aim for 1000+ samples per language\n",
    "2. **Balanced classes**: Equal samples per language\n",
    "3. **Data augmentation**: Add noise, typos, case variations\n",
    "4. **Longer training**: Increase epochs for larger datasets\n",
    "5. **Adjust architecture**: Increase filters for more languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c98039",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You have successfully trained a CNN-based language detection model. \n",
    "\n",
    "**What you've learned:**\n",
    "- Character-level text preprocessing\n",
    "- Building CNN architectures for text classification\n",
    "- Training and evaluating deep learning models\n",
    "- Making predictions with trained models\n",
    "\n",
    "**Next Steps:**\n",
    "- Add more languages to your dataset\n",
    "- Experiment with different hyperparameters\n",
    "- Try integrating with OCR for image-based language detection\n",
    "- Deploy as a web API or mobile application\n",
    "\n",
    "---\n",
    "\n",
    "*Created for the Language Detection Deep Learning Project*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
